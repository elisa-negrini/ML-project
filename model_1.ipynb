{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9602a01",
   "metadata": {},
   "source": [
    "# üîç Image Similarity with Pretrained CNN and Fine-Tuning\n",
    "Questo notebook esegue fine-tuning su un dataset etichettato di immagini, quindi usa il modello per recuperare le k immagini pi√π simili dalla gallery per ciascuna immagine di query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994d8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f717c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d84de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3991b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872d30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=5, lr=1e-4):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Loss: {running_loss/len(dataloader):.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce319ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extractor(trained_model):\n",
    "    feature_extractor = nn.Sequential(*list(trained_model.children())[:-1])\n",
    "    feature_extractor.eval()\n",
    "    return feature_extractor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8d73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_folder(folder_path, model):\n",
    "    image_paths = sorted([os.path.join(folder_path, fname)\n",
    "                          for fname in os.listdir(folder_path)\n",
    "                          if fname.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "    all_embeddings = []\n",
    "    filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), 32):\n",
    "            batch_paths = image_paths[i:i+32]\n",
    "            imgs = [transform(Image.open(p).convert(\"RGB\")) for p in batch_paths]\n",
    "            imgs = torch.stack(imgs).to(device)\n",
    "            vecs = model(imgs).squeeze(-1).squeeze(-1)\n",
    "            all_embeddings.append(vecs.cpu())\n",
    "            filenames.extend(batch_paths)\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0).numpy(), filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefcd0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_query_vs_gallery(query_embs, query_files, gallery_embs, gallery_files, k=5):\n",
    "    model = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "    model.fit(gallery_embs)\n",
    "    distances, indices = model.kneighbors(query_embs)\n",
    "\n",
    "    results = []\n",
    "    for i, query_path in enumerate(query_files):\n",
    "        query_rel = query_path.replace(\"\\\\\", \"/\")\n",
    "        gallery_matches = [gallery_files[idx].replace(\"\\\\\", \"/\") for idx in indices[i]]\n",
    "        results.append({\n",
    "            \"filename\": query_rel,\n",
    "            \"gallery_images\": gallery_matches\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef6ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(results, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171190b9",
   "metadata": {},
   "source": [
    "## üß™ Esecuzione completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c84a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utente\\anaconda3\\envs\\ElisaML\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\utente\\anaconda3\\envs\\ElisaML\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0739\n",
      "‚úÖ Submission salvata in: C:/Users/utente/Desktop/UNITN/Intro to ML/ML-project/submission/submission.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Fine-tune il modello sul training set\n",
    "train_dataset = datasets.ImageFolder(\"C:/Users/utente/Desktop/UNITN/Intro to ML/ML-project/Examples/training\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "model = get_model(num_classes=len(train_dataset.classes))\n",
    "model = train_model(model, train_loader, epochs=5)\n",
    "\n",
    "# Step 2: Estrai features da query e gallery\n",
    "feature_extractor = get_feature_extractor(model)\n",
    "query_embeddings, query_files = extract_embeddings_from_folder(\"C:/Users/utente/Desktop/UNITN/Intro to ML/ML-project/Examples/test/query\", feature_extractor)\n",
    "gallery_embeddings, gallery_files = extract_embeddings_from_folder(\"C:/Users/utente/Desktop/UNITN/Intro to ML/ML-project/Examples/test/gallery\", feature_extractor)\n",
    "\n",
    "# Step 3: Retrieval\n",
    "submission = retrieve_query_vs_gallery(query_embeddings, query_files, gallery_embeddings, gallery_files, k=3)\n",
    "\n",
    "# Step 4: Salvataggio nella repo\n",
    "submission_path = \"C:/Users/utente/Desktop/UNITN/Intro to ML/ML-project/submission/submission.json\"\n",
    "\n",
    "save_submission(submission, submission_path)\n",
    "print(f\"‚úÖ Submission salvata in: {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ElisaML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
