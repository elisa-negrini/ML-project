{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1tSM-zJXqtFBPRhZXhdq_RZN0yX-YZZHM",
      "authorship_tag": "ABX9TyPGe1ZXal+AwGtWeofHg4Xw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisa-negrini/ML-project/blob/main/model_DINO_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision==0.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ6NB8a3gl4j",
        "outputId": "1249339d-102c-413d-8dd5-8f7d7ba75323"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.0 (from versions: 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets # Rimosso transforms perché il processore gestisce tutto\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from transformers import AutoImageProcessor, AutoModel # Modificato per DINOv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Carica DINOv2 e il suo Image Processor\n",
        "MODEL_NAME = \"facebook/dinov2-base\"\n",
        "try:\n",
        "    dino_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "    image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante il caricamento del modello {MODEL_NAME}: {e}\")\n",
        "    print(\"Assicurati di avere una connessione internet e che il nome del modello sia corretto.\")\n",
        "    print(\"Potrebbe essere necessario installare/aggiornare transformers: pip install transformers --upgrade\")\n",
        "    # Esci o gestisci l'errore come preferisci se il modello non può essere caricato\n",
        "    exit()\n",
        "\n",
        "\n",
        "# La funzione get_transform non è più necessaria, image_processor gestisce le trasformazioni.\n",
        "\n",
        "# Classe per il fine-tuning che permette di aggiornare tutti i parametri\n",
        "class ImageClassifierFineTuner(nn.Module):\n",
        "    def __init__(self, base_model, embed_dim, num_classes, unfreeze_layers=True):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        # Il classificatore prende in input le feature estratte dal modello base\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Se vogliamo fare fine-tuning, sblocchiamo i parametri del modello base\n",
        "        if unfreeze_layers:\n",
        "            for param in self.base_model.parameters(): # Sblocca tutti i parametri del base_model\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            # Altrimenti, congela il modello base (solo feature extraction)\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Assicurati che il classificatore sia sempre addestrabile\n",
        "        for param in self.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # Estrae le feature dal modello base.\n",
        "        # Per DINOv2, usiamo l'output dell'ultimo strato nascosto, prendendo il token [CLS]\n",
        "        # Non usiamo torch.no_grad() qui per permettere il backpropagation attraverso il base_model se sbloccato\n",
        "        outputs = self.base_model(pixel_values=pixel_values)\n",
        "        # features = outputs.pooler_output # Alcuni modelli hanno pooler_output\n",
        "        features = outputs.last_hidden_state[:, 0, :] # Prende l'embedding del token [CLS]\n",
        "        return self.classifier(features)\n",
        "\n",
        "def custom_collate(batch):\n",
        "    # Filter out PIL Images and collate the rest\n",
        "    tensors, labels = [], []\n",
        "    for image, label in batch:\n",
        "        tensors.append(image)  # Store PIL Images separately\n",
        "        labels.append(label)\n",
        "\n",
        "    # Collate labels using default collation\n",
        "    labels = default_collate(labels)\n",
        "\n",
        "    return tensors, labels  # Return PIL Images and collated labels\n",
        "\n",
        "def train_model(model, dataloader, image_processor_func, epochs=5, lr=5e-5):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Creiamo due gruppi di parametri con learning rate diversi\n",
        "    # Filtra i parametri che effettivamente richiedono gradiente\n",
        "    base_params = [p for n, p in model.named_parameters() if \"base_model\" in n and p.requires_grad]\n",
        "    classifier_params = [p for n, p in model.named_parameters() if \"classifier\" in n and p.requires_grad]\n",
        "\n",
        "    # Controlla se ci sono parametri da ottimizzare nel base_model\n",
        "    optimizer_params = []\n",
        "    if base_params:\n",
        "        optimizer_params.append({'params': base_params, 'lr': lr / 10}) # LR più basso per il modello pre-addestrato\n",
        "    if classifier_params:\n",
        "        optimizer_params.append({'params': classifier_params, 'lr': lr}) # LR standard per il classificatore\n",
        "    else: # Se solo il base_model è sbloccato (improbabile ma per sicurezza)\n",
        "        if not base_params:\n",
        "             print(\"Attenzione: Nessun parametro da addestrare!\")\n",
        "             return model # Non c'è nulla da addestrare\n",
        "\n",
        "    optimizer = torch.optim.AdamW(optimizer_params)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(dataloader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for pil_images, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            # Le immagini dal DataLoader sono ora PIL Images\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Preprocessing con l'image_processor specifico del modello (es. DINOv2)\n",
        "            # image_processor_func gestisce la conversione a tensori, resize, crop, normalizzazione\n",
        "            try:\n",
        "                inputs = image_processor_func(images=pil_images, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "            except Exception as e:\n",
        "                print(f\"Errore durante il processing delle immagini: {e}\")\n",
        "                # Potresti voler ispezionare una delle pil_images qui\n",
        "                # print(f\"Tipo immagine: {type(pil_images[0]) if pil_images else 'N/A'}\")\n",
        "                continue # Salta questo batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.pixel_values) # Passa i pixel_values processati\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calcolo dell'accuratezza\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
        "        epoch_acc = (100.0 * correct / total) if total > 0 else 0\n",
        "        print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_feature_extractor(trained_model, image_processor_func):\n",
        "    # trained_model è l'istanza di ImageClassifierFineTuner dopo l'addestramento\n",
        "    # Vogliamo estrarre le feature dal *base_model* al suo interno\n",
        "    feature_extractor_model = trained_model.base_model\n",
        "    feature_extractor_model.eval() # Assicuriamoci che il modello base sia in modalità valutazione\n",
        "\n",
        "    def extractor(image_paths):\n",
        "        embs = []\n",
        "        for path in tqdm(image_paths, desc=\"Extracting features\"):\n",
        "            try:\n",
        "                image = Image.open(path).convert(\"RGB\")\n",
        "                # Processa l'immagine usando l'image_processor\n",
        "                inputs = image_processor_func(images=image, return_tensors=\"pt\").to(device)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Attenzione: File immagine non trovato {path}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Errore nell'aprire o processare l'immagine {path}: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Estrae le feature usando il base_model (es. DINOv2)\n",
        "                outputs = feature_extractor_model(pixel_values=inputs.pixel_values)\n",
        "                # emb = outputs.pooler_output # Se DINOv2 avesse un pooler_output significativo per questo task\n",
        "                emb = outputs.last_hidden_state[:, 0, :] # Prende l'embedding del token [CLS]\n",
        "                embs.append(emb.cpu().numpy()[0])\n",
        "\n",
        "        if not embs: # Se nessuna embedding è stata estratta (es. tutti i file mancanti)\n",
        "            return np.array([]).astype(\"float32\") # Restituisce un array vuoto con il tipo corretto\n",
        "        return np.array(embs).astype(\"float32\")\n",
        "    return extractor\n",
        "\n",
        "def retrieve_query_vs_gallery(query_embs, query_files, gallery_embs, gallery_files, k=5):\n",
        "    if query_embs.shape[0] == 0 or gallery_embs.shape[0] == 0:\n",
        "        print(\"Attenzione: Non ci sono embedding per query o gallery. Impossibile eseguire il retrieval.\")\n",
        "        return []\n",
        "\n",
        "    model_nn = NearestNeighbors(n_neighbors=min(k, gallery_embs.shape[0]), metric='cosine') # k non può essere > num_samples\n",
        "    model_nn.fit(gallery_embs)\n",
        "    distances, indices = model_nn.kneighbors(query_embs)\n",
        "\n",
        "    results = []\n",
        "    for i, query_path in enumerate(query_files):\n",
        "        query_rel = query_path.replace(\"\\\\\", \"/\")\n",
        "        gallery_matches = [gallery_files[idx].replace(\"\\\\\", \"/\") for idx in indices[i]]\n",
        "        results.append({\n",
        "            \"filename\": query_rel,\n",
        "            \"gallery_images\": gallery_matches\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def save_submission(results, output_path):\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "# ===============================\n",
        "# ESECUZIONE COMPLETA\n",
        "# ===============================\n",
        "\n",
        "# Step 1: Prepara il dataset. ImageFolder caricherà le immagini come PIL.\n",
        "# Nessuna trasformazione specificata qui, così ImageFolder restituisce immagini PIL.\n",
        "# Le trasformazioni (resize, crop, normalizzazione) saranno gestite da image_processor.\n",
        "try:\n",
        "    train_dataset = datasets.ImageFolder(\"drive/MyDrive/Testing_images4/training\") # Carica PIL Images\n",
        "    if not train_dataset.classes:\n",
        "        print(\"Errore: Nessuna classe trovata nel dataset di training. Controlla il percorso e la struttura della cartella.\")\n",
        "        exit()\n",
        "    num_classes = len(train_dataset.classes)\n",
        "    print(f\"Trovate {num_classes} classi: {train_dataset.classes}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Errore: Cartella di training 'drive/MyDrive/Testing_images4/training' non trovata.\")\n",
        "    print(\"Assicurati che il percorso sia corretto e che Google Drive sia montato se usi Colab.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante il caricamento del dataset di training: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# DataLoader ora gestirà batch di immagini PIL e labels\n",
        "# Il collate_fn di default dovrebbe funzionare bene, raggruppando le immagini PIL in una lista.\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate) # Utilizzo della funzione custom_collate\n",
        "\n",
        "# Step 2: Fine-tune DINOv2\n",
        "# La dimensione dell'embedding per dinov2-base è 768\n",
        "embed_dim = dino_model.config.hidden_size # Più robusto che hardcodare 768\n",
        "model_to_train = ImageClassifierFineTuner(dino_model, embed_dim=embed_dim, num_classes=num_classes, unfreeze_layers=True)\n",
        "\n",
        "print(\"Inizio fine-tuning del modello...\")\n",
        "# Passiamo image_processor alla funzione train_model per processare i batch di immagini PIL\n",
        "# Aumentato numero di epoche, LR potrebbe necessitare di tuning per DINOv2\n",
        "trained_fine_tuned_model = train_model(model_to_train, train_loader, image_processor_func=image_processor, epochs=10, lr=1e-5) # LR ridotto per DINOv2\n",
        "\n",
        "# Step 3: Estrai features da query e gallery usando il modello fine-tuned\n",
        "# Passiamo il modello fine-tuned completo e l'image_processor\n",
        "# get_feature_extractor accederà a trained_fine_tuned_model.base_model internamente\n",
        "feature_extractor_fn = get_feature_extractor(trained_fine_tuned_model, image_processor_func=image_processor)\n",
        "\n",
        "query_folder = \"drive/MyDrive/Testing_images4/test/query\"\n",
        "gallery_folder = \"drive/MyDrive/Testing_images4/test/gallery\"\n",
        "\n",
        "try:\n",
        "    query_files = [os.path.join(query_folder, fname) for fname in os.listdir(query_folder) if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "    gallery_files = [os.path.join(gallery_folder, fname) for fname in os.listdir(gallery_folder) if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "\n",
        "    if not query_files:\n",
        "        print(f\"Attenzione: Nessun file immagine trovato in {query_folder}\")\n",
        "    if not gallery_files:\n",
        "        print(f\"Attenzione: Nessun file immagine trovato in {gallery_folder}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Errore: Cartella query o gallery non trovata. Controlla i percorsi: {e}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante la lettura dei file query/gallery: {e}\")\n",
        "    exit()\n",
        "\n",
        "if not query_files or not gallery_files:\n",
        "    print(\"Nessun file query o gallery da processare. Uscita.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Trovati {len(query_files)} file query e {len(gallery_files)} file gallery.\")\n",
        "\n",
        "query_embs = feature_extractor_fn(query_files)\n",
        "gallery_embs = feature_extractor_fn(gallery_files)\n",
        "\n",
        "# Step 4: Retrieval\n",
        "if query_embs.shape[0] > 0 and gallery_embs.shape[0] > 0:\n",
        "    submission = retrieve_query_vs_gallery(query_embs, query_files, gallery_embs, gallery_files, k=20)\n",
        "    # Step 5: Salvataggio\n",
        "    submission_path = \"drive/MyDrive/Testing_images4/submission/submission_dinov2.json\" # Nome file modificato\n",
        "    save_submission(submission, submission_path)\n",
        "    print(f\"✅ Submission salvata in: {submission_path}\")\n",
        "else:\n",
        "    print(\"Nessuna embedding estratta, impossibile eseguire il retrieval o salvare la submission.\")\n",
        "\n",
        "print(\"Esecuzione completata.\")\n"
      ],
      "metadata": {
        "id": "AyJnr1rQBIZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d172c3-6ce3-4838-f75b-b78611f7481b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trovate 6 classi: ['bicycle', 'bus', 'car', 'motorcycle', 'train', 'tram']\n",
            "Inizio fine-tuning del modello...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/10:   0%|          | 0/17 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:42: UserWarning: The following named arguments are not valid for `BitImageProcessor.preprocess` and were ignored: 'padding', 'truncation'\n",
            "  return self.preprocess(images, **kwargs)\n",
            "Epoch 1/10:  12%|█▏        | 2/17 [00:04<00:29,  1.97s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 1/10: 100%|██████████| 17/17 [00:30<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 1.6807, Accuracy: 33.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 17/17 [00:31<00:00,  1.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 0.8371, Accuracy: 76.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 17/17 [00:31<00:00,  1.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Loss: 0.4477, Accuracy: 95.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 17/17 [00:30<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Loss: 0.2696, Accuracy: 98.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 17/17 [00:31<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Loss: 0.1847, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 17/17 [00:32<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Loss: 0.1366, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 17/17 [00:32<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Loss: 0.1200, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 17/17 [00:30<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Loss: 0.1097, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 17/17 [00:31<00:00,  1.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Loss: 0.1037, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 17/17 [00:31<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Loss: 0.1047, Accuracy: 100.00%\n",
            "Trovati 6 file query e 130 file gallery.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 6/6 [00:00<00:00, 15.07it/s]\n",
            "Extracting features: 100%|██████████| 130/130 [00:12<00:00, 10.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Submission salvata in: drive/MyDrive/Testing_images4/submission/submission_dinov2.json\n",
            "Esecuzione completata.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}