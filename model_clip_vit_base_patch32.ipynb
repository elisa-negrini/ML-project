{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisa-negrini/ML-project/blob/main/ML1clip_vit_base_patch32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyJnr1rQBIZS",
        "outputId": "61a7e724-8e05-4e6f-f94a-287282854086"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\miklo\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Epoch 1/10:  47%|████▋     | 8/17 [00:32<00:34,  3.79s/it]c:\\Users\\miklo\\anaconda3\\envs\\myenv\\Lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 1/10: 100%|██████████| 17/17 [01:07<00:00,  3.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Loss: 1.2284, Accuracy: 66.15%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 17/17 [01:09<00:00,  4.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Loss: 0.3704, Accuracy: 99.62%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 17/17 [01:07<00:00,  3.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Loss: 0.1167, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 17/17 [01:08<00:00,  4.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Loss: 0.0553, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 17/17 [01:09<00:00,  4.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Loss: 0.0367, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 17/17 [01:07<00:00,  3.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Loss: 0.0282, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 17/17 [01:07<00:00,  3.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Loss: 0.0248, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 17/17 [01:06<00:00,  3.93s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 - Loss: 0.0235, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 17/17 [01:06<00:00,  3.93s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 - Loss: 0.0223, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 17/17 [01:06<00:00,  3.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 - Loss: 0.0222, Accuracy: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 6/6 [00:01<00:00,  5.68it/s]\n",
            "Extracting features: 100%|██████████| 130/130 [00:25<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Submission salvata in: testing_images4/submission/submission.json\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Carica CLIP\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Usiamo direttamente le trasformazioni di CLIP\n",
        "def get_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        # Non applichiamo normalizzazione qui, lasciamo che clip_processor la gestisca\n",
        "    ])\n",
        "\n",
        "# Classe per il fine-tuning che permette di aggiornare tutti i parametri\n",
        "class CLIPFineTuner(nn.Module):\n",
        "    def __init__(self, base_model, embed_dim, num_classes, unfreeze_layers=True):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Se vogliamo fare fine-tuning, sblocchiamo i parametri del modello base\n",
        "        if unfreeze_layers:\n",
        "            for param in self.base_model.vision_model.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            # Altrimenti, congela il modello base (solo feature extraction)\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # Non usiamo torch.no_grad() qui per permettere il backpropagation\n",
        "        features = self.base_model.get_image_features(pixel_values=pixel_values)\n",
        "        return self.classifier(features)\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=5e-5):\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Creiamo due gruppi di parametri con learning rate diversi\n",
        "    base_params = [p for n, p in model.named_parameters() if \"base_model\" in n]\n",
        "    classifier_params = [p for n, p in model.named_parameters() if \"classifier\" in n]\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': base_params, 'lr': lr / 10},  # LR più basso per il modello pre-addestrato\n",
        "        {'params': classifier_params, 'lr': lr}   # LR standard per il classificatore\n",
        "    ])\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(dataloader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Preprocessing con clip_processor, specificiamo do_rescale=False\n",
        "            # poiché le immagini sono già in formato [0,1] da transforms.ToTensor()\n",
        "            inputs = clip_processor(images=images, return_tensors=\"pt\", do_rescale=False).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.pixel_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calcolo dell'accuratezza\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_acc = 100.0 * correct / total\n",
        "        print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_feature_extractor(model):\n",
        "    def extractor(image_paths):\n",
        "        model.eval()  # Assicuriamoci che il modello sia in modalità valutazione\n",
        "        embs = []\n",
        "        for path in tqdm(image_paths, desc=\"Extracting features\"):\n",
        "            image = Image.open(path).convert(\"RGB\")\n",
        "            inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                # Usiamo direttamente il modello fine-tuned per estrarre features\n",
        "                emb = model.base_model.get_image_features(**inputs)\n",
        "            embs.append(emb.cpu().numpy()[0])\n",
        "        return np.array(embs).astype(\"float32\")\n",
        "    return extractor\n",
        "\n",
        "def retrieve_query_vs_gallery(query_embs, query_files, gallery_embs, gallery_files, k=5):\n",
        "    model = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
        "    model.fit(gallery_embs)\n",
        "    distances, indices = model.kneighbors(query_embs)\n",
        "\n",
        "    results = []\n",
        "    for i, query_path in enumerate(query_files):\n",
        "        query_rel = query_path.replace(\"\\\\\", \"/\")\n",
        "        gallery_matches = [gallery_files[idx].replace(\"\\\\\", \"/\") for idx in indices[i]]\n",
        "        results.append({\n",
        "            \"filename\": query_rel,\n",
        "            \"gallery_images\": gallery_matches\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def save_submission(results, output_path):\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "# ===============================\n",
        "# ESECUZIONE COMPLETA\n",
        "# ===============================\n",
        "\n",
        "# Step 1: Prepara il dataset con le trasformazioni corrette\n",
        "transform = get_transform()\n",
        "train_dataset = datasets.ImageFolder(\"testing_images4/training\", transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Batch size ridotto per evitare OOM\n",
        "\n",
        "# Step 2: Fine-tune CLIP\n",
        "model = CLIPFineTuner(clip_model, embed_dim=512, num_classes=len(train_dataset.classes), unfreeze_layers=True)\n",
        "model = train_model(model, train_loader, epochs=10, lr=1e-4)  # Aumentato numero di epoche\n",
        "\n",
        "# Step 3: Estrai features da query e gallery usando il modello fine-tuned\n",
        "extractor = get_feature_extractor(model)\n",
        "\n",
        "query_folder = \"testing_images4/test/query\"\n",
        "gallery_folder = \"testing_images4/test/gallery\"\n",
        "query_files = [os.path.join(query_folder, fname) for fname in os.listdir(query_folder) if fname.endswith(\".jpg\")]\n",
        "gallery_files = [os.path.join(gallery_folder, fname) for fname in os.listdir(gallery_folder) if fname.endswith(\".jpg\")]\n",
        "\n",
        "query_embs = extractor(query_files)\n",
        "gallery_embs = extractor(gallery_files)\n",
        "\n",
        "# Step 4: Retrieval\n",
        "submission = retrieve_query_vs_gallery(query_embs, query_files, gallery_embs, gallery_files, k=20)\n",
        "\n",
        "# Step 5: Salvataggio\n",
        "submission_path = \"testing_images4/submission/submission.json\"\n",
        "save_submission(submission, submission_path)\n",
        "print(f\"✅ Submission salvata in: {submission_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNyUN4Q9YmPz9H6AT5d/mio",
      "include_colab_link": true,
      "mount_file_id": "1nNoaWqh2dMjAeP8KYezAmk_hNwaR9eQK",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
